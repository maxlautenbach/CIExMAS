{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup",
   "id": "d180eaa2091349c8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T16:31:10.513752Z",
     "start_time": "2025-03-18T16:30:59.061838Z"
    }
   },
   "source": [
    "import helper_tools.parser as parser\n",
    "import importlib\n",
    "import pandas as pd\n",
    "\n",
    "importlib.reload(parser)\n",
    "\n",
    "relation_df, entity_df, docs = parser.synthie_parser(\"train\")\n",
    "entity_set = entity_df[['entity', 'entity_uri']].drop_duplicates()\n",
    "predicate_set_df = relation_df[[\"predicate\", \"predicate_uri\"]].drop_duplicates()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 27 files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f6392d3c76b4977901b97ac93d74a9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2970.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Entities to Qdrant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:05<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Predicates to Qdrant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  8.91it/s]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:03:59.367518Z",
     "start_time": "2025-03-13T16:03:59.331559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langfuse.callback import CallbackHandler\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\"),\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model_name=\"Meta-Llama-3.3-70B-Instruct\", base_url=\"https://api.sambanova.ai/v1\", api_key=os.getenv(\"SAMBANOVA_API_KEY\"))\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')"
   ],
   "id": "6d357659da5e4afd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:31:10.538774Z",
     "start_time": "2025-03-18T16:31:10.534666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_doc = docs.iloc[1]\n",
    "doc_id = target_doc[\"docid\"]\n",
    "text = target_doc[\"text\"]\n",
    "text"
   ],
   "id": "2528cff98f514fda",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ricardo Lumengo is a Swiss politician. He was born in Fribourg and lives in Biel/Bienne. He works in Bern and speaks the Kongo language.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T07:50:56.426991Z",
     "start_time": "2025-03-14T07:50:55.891535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "label_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"wikidata_labels\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "description_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"wikidata_descriptions\",\n",
    "    embedding=embeddings\n",
    ")"
   ],
   "id": "703414f8d466629d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:14:15.335010Z",
     "start_time": "2025-03-14T09:14:15.309418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.types import Command\n",
    "from typing import TypedDict, Literal\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import re\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "class cIEState(TypedDict):\n",
    "    text: str\n",
    "    call_trace: list[tuple[str]]\n",
    "    results: list[str]\n",
    "    comments: list[AIMessage]\n",
    "    instruction: str\n",
    "    \n",
    "def planner(state: cIEState) -> Command[Literal[\"agent_instructor_agent\"]]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are an expert in planning and executing tasks within multi-agent systems. Your role is to design and refine a detailed plan that processes a given text into a triple format, specifically for closed information extraction using an underlying Knowledge Graph. You design the plan for the agent instructor agent, which should execute your plan, call and instruct agents. It is only able to execute one step at a time. Your plan must be based on the following inputs:\n",
    "    - Agent Call Trace\n",
    "    - Agent Comments\n",
    "    - The provided input text\n",
    "    - All intermediate results produced during the process\n",
    "    \n",
    "    For executing the tasks, you can include the following agents in the plan:\n",
    "    - **Entity Extraction Agent:** Can extract entities from the text.\n",
    "    - **Relation Extraction Agent:** Can extract relations from the text.\n",
    "    - **URI Detection Agent:** Based on search terms, can determine if there is an associated entity or relation in the Knowledge Graph.\n",
    "    - **Result Formatting Agent:** After executing and iterating over the task, the result formatting agent should be called to summarize the results and output the final triples. Calling this agent will end the processing.\n",
    "    \n",
    "    Your plan should clearly outline the steps required to achieve the goal, ensuring that each phase is actionable and verifiable. The plan will be passed to the Agent Instructor, who will execute the steps through a series of Agent Calls. You will be asked to build up a plan, as long as no final result is done. Your response should be precise, structured, and demonstrate deep expertise in orchestrating complex multi-agent systems for closed Information Extraction tasks. Please line up the plan that you have, to accomplish the task. Do not include tasks that are already worked on.\n",
    "    \n",
    "    If you are called for the first time write down the full plan. If you are called afterwards just say what the next task is and where in your plan we are.\n",
    "    \n",
    "    Please base your plan on the following information:\n",
    "    \n",
    "    Agent Call Trace: {call_trace}\n",
    "    Agent Comments: {comments}\n",
    "    The provided input text: {text}\n",
    "    All intermediate results produced during the process: {results}\n",
    "    \"\"\")\n",
    "   \n",
    "    response_chain = prompt | model\n",
    "    \n",
    "    response = response_chain.invoke(state)\n",
    "    \n",
    "    next_agent = \"agent_instructor_agent\"\n",
    "    \n",
    "    if \"<FINISH_MAS>\" in response.content:\n",
    "        next_agent = END\n",
    "          \n",
    "    return Command(goto=next_agent, update={\"comments\": state[\"comments\"] + [response]})\n",
    "\n",
    "def agent_instructor_agent(state: cIEState) -> Command[Literal[\"entity_extraction_agent\", \"relation_extraction_agent\", \"uri_detection_agent\", \"result_formatting_agent\"]]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    \n",
    "     You are an expert for executing plans in multi-agent-systems and instructing agents. You are embedded within such a MAS with the final goal of processing a text into relations. You will receive a plan from a planning agent within the agent comments alongside with the feedback given by the result checker. In addition, you will receive your agent call traces and the text which is being processed. Your task is then to reason, how the next agent should be called. The planner might give you a hint, which agent should be called next. \n",
    "     \n",
    "    You have access on the following agents:\n",
    "    Entity Extraction Agent\n",
    "    - id: entity_extraction_agent\n",
    "    - use of instruction: The use of an instruction is optional. It will be included in the context of the prompt of the agent and can modify the agents behaviour. Please do not include the original text in the prompt.\n",
    "    - description: Can extract entities from the text.\n",
    "    - state access on: text, instruction\n",
    "    \n",
    "    Relation Extraction Agent\n",
    "    - id: relation_extraction_agent\n",
    "    - use of instruction: The use of an instruction is optional. It will be included in the context of the prompt of the agent and can modify the agents behaviour. Please do not include the original text in the prompt. It can be relevant to provide the relation extraction agent with already extracted entities or entities it should focus on.\n",
    "    - description: Can extract relations from the text.\n",
    "    - state access on: text, instruction\n",
    "    \n",
    "    URI Detection Agent\n",
    "    - id: uri_detection_agent\n",
    "    - use of instruction: The use of an instruction is mandatory. The instruction must be a comma separated list of search terms. For each search term include the search mode - either [LABEL] for a search on rdfs:label or [DESCR] for a search on the description of an entity/relation.\n",
    "    - description: Based on search terms, can determine if there is an associated entity or relation in the Knowledge Graph. The agent will respond with a mapping of search terms to URIs\n",
    "    - state access on: instruction\n",
    "    \n",
    "    Result Formatting Agent\n",
    "    - id: result_formatting_agent\n",
    "    - use of instruction: None\n",
    "    - description: Utilizes the whole state to output the final triples in an appropriate format\n",
    "    - state access on: call_trace, comments, text, results\n",
    "     \n",
    "    Please include in your response exact one agent call using the following agent call structure:\n",
    "    \n",
    "    <agent_call>\n",
    "        <id>AGENT_ID</id>\n",
    "        <instruction>Put your instructions for the agents here</instruction>\n",
    "    <agent_call/>\n",
    "    \n",
    "    \n",
    "    \n",
    "    Agent Call Trace: {call_trace}\n",
    "    Agent Comments: {comments}\n",
    "    The provided input text: {text}\n",
    "    All intermediate results produced during the process: {results}\n",
    "    \n",
    "    \"\"\")\n",
    "    \n",
    "    response_chain = prompt | model\n",
    "    \n",
    "    response = response_chain.invoke(state)\n",
    "    \n",
    "    agent_id_match = re.search(r'<id>(.*?)</id>', response.content, re.DOTALL)\n",
    "    if agent_id_match:\n",
    "        agent_id = agent_id_match.group(1)\n",
    "    else:\n",
    "        agent_id = \"agent_instructor\"\n",
    "        \n",
    "    instruction_match = re.search(r'<instruction>(.*?)</instruction>', response.content, re.DOTALL)\n",
    "    if instruction_match:\n",
    "        instruction = instruction_match.group(1)\n",
    "    else:\n",
    "        instruction = \"\"\n",
    "        \n",
    "    return Command(goto=agent_id, update={\"instruction\": instruction, \"call_trace\": state[\"call_trace\"] + [(agent_id, instruction)]})\n",
    "\n",
    "def entity_extraction_agent(state: cIEState) -> Command[Literal[\"result_checker_agent\"]]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    \n",
    "    You are an expert for entity extraction out of text in a multi-agent-system for closed information extraction. You will receive a text out of the state from which you should extract all entities. In addition, the agent_instructor might give you an instruction, which you should follow. Your task is then to follow the optional instruction as well as this system prompt and return a comma separated list of entities that are in the text, which is enclosed in <result>insert list here</result>. \n",
    "    \n",
    "    The provided input text: {text}\n",
    "    Instruction: {instruction}\n",
    "    \n",
    "    \"\"\")\n",
    "    \n",
    "    response_chain = prompt | model\n",
    "    \n",
    "    response = response_chain.invoke(state)\n",
    "    \n",
    "    result_match = re.search(r'<result>(.*?)</result>', response.content, re.DOTALL)\n",
    "    if result_match:\n",
    "        result = result_match.group(1)\n",
    "    else:\n",
    "        result = \"\"\n",
    "    \n",
    "    result = f\"Output of entity_extraction_agent: {result}\"\n",
    "    \n",
    "    return Command(goto=\"result_checker_agent\", update={\"instruction\": \"\", \"results\": state[\"results\"] + [result]})\n",
    "\n",
    "def relation_extraction_agent(state: cIEState) -> Command[Literal[\"result_checker_agent\"]]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    \n",
    "    You are an expert for relation extraction out of text in a multi-agent-system for closed information extraction. You will receive a text out of the state from which you should extract all relation. As closed information extraction uses an underlying knowledge graph, there can be different names for similar predicates. Therefore, extract also alternative predicates, when applicable (i.e. Berlin, located in, Germany -> Berlin, country, Germany). \n",
    "     \n",
    "    In addition, the agent_instructor might give you an instruction, which you should follow. Your task is then to follow the optional instruction as well as this system prompt and return a list of all triples, where each triple is enclosed in <triple> tags and subject, predicate and object are comma separated from each other. Enclose your pure result in <result> tags\n",
    "    \n",
    "    The provided input text: {text}\n",
    "    Instruction: {instruction}\n",
    "    \n",
    "    \"\"\")\n",
    "    \n",
    "    response_chain = prompt | model\n",
    "    \n",
    "    response = response_chain.invoke(state)\n",
    "    \n",
    "    result_match = re.search(r'<result>(.*?)</result>', response.content, re.DOTALL)\n",
    "    if result_match:\n",
    "        result = result_match.group(1)\n",
    "    else:\n",
    "        result = \"\"\n",
    "        \n",
    "    result = f\"Output of relation_extraction_agent: {result}\"\n",
    "    \n",
    "    return Command(goto=\"result_checker_agent\", update={\"instruction\": \"\", \"results\": state[\"results\"] + [result]})\n",
    "\n",
    "def uri_detection_agent(state: cIEState) -> Command[Literal[\"result_checker_agent\"]]:\n",
    "    search_terms = state[\"instruction\"].split(\",\")\n",
    "    label_search_terms = [term.replace(\"[LABEL]\",\"\") for term in search_terms if \"[LABEL]\" in term]\n",
    "    description_search_terms = [term.replace(\"[DESCR]\",\"\") for term in search_terms if \"[DESCR]\" in term]\n",
    "    response = \"\"\n",
    "    for term in label_search_terms:\n",
    "        response += f'Most Similar rdfs:label Search Results for {term}:{[{\"label\": doc.page_content, \"uri\": doc.metadata[\"uri\"]} for doc in label_vector_store.similarity_search(term, k=3)]}\\n\\n'\n",
    "    for term in description_search_terms:\n",
    "        response += f'Most Similar rdfs:label Search Results for {term}:{[{\"label\": doc.page_content, \"uri\": doc.metadata[\"uri\"]} for doc in description_vector_store.similarity_search(term, k=3)]}\\n\\n'\n",
    "    response = response.replace(\"},\", \"},\\n\")\n",
    "        \n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a formatting agent. Your task is to check and format the output of the URI detection tool. The tool will give a response like this:\n",
    "        Most Similar Detection Result for Olaf Scholz: ('label': Angela Merkel, 'uri': 'http://www.wikidata.org/entity/Q567)\n",
    "        \n",
    "        Your task is to check the response and output an overall mapping of search terms to URIs. If something doesn't match, please response the non mapping search term with the advise, that those might not be present in the knowledge graph. Please also leverage the text for identifying the context of the search terms.\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        URI detection tool response:\n",
    "        \n",
    "        {response}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt_template | model\n",
    "    result = chain.invoke({\"response\": response, \"text\": state[\"text\"]})\n",
    "    \n",
    "    result = f\"Output of uri_detection_agent: {result.content}\"\n",
    "    \n",
    "    return Command(goto=\"result_checker_agent\", update={\"instruction\": \"\", \"results\": state[\"results\"] + [result]})\n",
    "\n",
    "def result_checker_agent(state: cIEState) -> Command[Literal[\"planner\"]]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are an expert in monitoring multi-agent-systems. In this case you are giving feedback on the process to the planning agent. Therefore, you can see the plans made, as well as agent calls and the history of comments. In addition, you will have access to a text, that should be transformed into triplets, which can be inserted into an underlying knowledge graph. This task often requires multiple iterations to really catch every entity and relation especially those, that are not visible first glimpse. As long as you think the result can be improved, just response with your feedback, which will be processed by the planner in the next step. Really push the result to the edge, what an LLM can do.\n",
    "    \n",
    "    Agent Call Trace: {call_trace}\n",
    "    Agent Comments: {comments}\n",
    "    The provided input text: {text}\n",
    "    All intermediate results produced during the process: {results}\n",
    "    \"\"\")\n",
    "    \n",
    "    response_chain = prompt | model\n",
    "    \n",
    "    response = response_chain.invoke(state)\n",
    "          \n",
    "    return Command(goto=\"planner\", update={\"comments\": state[\"comments\"] + [response]})\n",
    "\n",
    "def result_formatting_agent(state: cIEState) -> Command[Literal[END]]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are an expert in formatting results of multi-agent-systems, which are used for closed information extraction. Therefore, your task is to produce triples in turtle format, that can be inserted in the underlying knowledge graph. Therefore, you will get access to the full state of the multi-agent-system including the full call trace, the comments of the planner and the result checker, the provided input text and all intermediate results. Please note, that the so called relation extraction agent will output more triples than necessary due to prompting. Please reduce the output so, that no triple is a duplicate of another. Please do not extract predicate from the rdf or rdfs namespaces. Please only use the http://www.wikidata.org/entity/ namespace.\n",
    "    \n",
    "    If you want to incorporate reasoning in your output make sure that you enclose the turtle output in <ttl> tags, so that it can be extracted afterwards.\n",
    "    \n",
    "    Agent Call Trace: {call_trace}\n",
    "    Agent Comments: {comments}\n",
    "    The provided input text: {text}\n",
    "    All intermediate results produced during the process: {results}\n",
    "    \"\"\")\n",
    "    \n",
    "    response_chain = prompt | model\n",
    "    \n",
    "    response = response_chain.invoke(state, config={\"callbacks\": [langfuse_handler]})\n",
    "    \n",
    "    result_match = re.search(r'<ttl>(.*?)</ttl>', response.content, re.DOTALL)\n",
    "\n",
    "    if result_match:\n",
    "        result = result_match.group(1)\n",
    "    else:\n",
    "        result = \"\"\n",
    "    \n",
    "    return Command(goto=END, update={\"results\": state[\"results\"] + [result]})\n",
    "\n",
    "builder = StateGraph(cIEState)\n",
    "builder.add_node(planner)\n",
    "builder.add_node(agent_instructor_agent)\n",
    "builder.add_node(entity_extraction_agent)\n",
    "builder.add_node(relation_extraction_agent)\n",
    "builder.add_node(uri_detection_agent)\n",
    "builder.add_node(result_checker_agent)\n",
    "builder.add_node(result_formatting_agent)\n",
    "\n",
    "builder.add_edge(START, \"planner\")\n",
    "\n",
    "graph = builder.compile()"
   ],
   "id": "41fa1adc21242777",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:14:40.768686Z",
     "start_time": "2025-03-14T09:14:15.988870Z"
    }
   },
   "cell_type": "code",
   "source": "response_state = graph.invoke({\"text\": text, \"results\": [], \"call_trace\": [], \"comments\": []}, config={\"callbacks\": [langfuse_handler]})",
   "id": "a4c4f50fe945aab7",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save response state",
   "id": "7b39e0def9d838d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:09:48.920011Z",
     "start_time": "2025-03-14T09:09:48.912891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(response_state, open(\"./state_storage/final.state\", \"wb\"))"
   ],
   "id": "95f08ad33961a45b",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pretty Print Response State",
   "id": "9b1a6616dc62b977"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:14:40.788876Z",
     "start_time": "2025-03-14T09:14:40.780157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"\"\"cIE for text: {response_state[\"text\"]}\n",
    "\n",
    "Results:\"\"\")\n",
    "for i, call in enumerate(response_state[\"call_trace\"]):\n",
    "    print(f\"Agent ID: {call[0]}\")\n",
    "    print(f\"Instruction: {call[1]}\")\n",
    "    print(f\"Result: {response_state['results'][i]}\\n\\n\")\n",
    "    \n",
    "print(\"Agent Comments:\")\n",
    "for comment in response_state[\"comments\"]:\n",
    "    print(\"-- START OF OUTPUT --\\n\" + comment.content + \"\\n -- END OF OUTPUT --\\n\")"
   ],
   "id": "fd18d19f9fdafd1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cIE for text: Corfe Castle railway station is a station on the Swanage Railway in the village of Corfe Castle, in the United Kingdom.\n",
      "\n",
      "Results:\n",
      "Agent ID: entity_extraction_agent\n",
      "Instruction: \n",
      "Result: Output of entity_extraction_agent: \n",
      "\n",
      "\n",
      "Agent ID: entity_extraction_agent\n",
      "Instruction: Extract entities from the preprocessed text: \"Corfe Castle railway station is a station on the Swanage Railway in the village of Corfe Castle, in the United Kingdom.\"\n",
      "Result: Output of entity_extraction_agent: Corfe Castle railway station, Swanage Railway, Corfe Castle, United Kingdom\n",
      "\n",
      "\n",
      "Agent ID: uri_detection_agent\n",
      "Instruction: Corfe Castle railway station[LABEL], Swanage Railway[LABEL], Corfe Castle[LABEL], United Kingdom[LABEL], located on[DESCR], serves[DESCR]\n",
      "Result: Output of uri_detection_agent: After analyzing the response from the URI detection tool, I have identified the most relevant mappings between search terms and URIs. Here is the overall mapping:\n",
      "\n",
      "* Corfe Castle railway station: http://www.wikidata.org/entity/Q5170476\n",
      "* Swanage Railway: http://www.wikidata.org/entity/Q7653559\n",
      "* Corfe Castle: http://www.wikidata.org/entity/Q1236511\n",
      "* United Kingdom: http://www.wikidata.org/entity/Q145\n",
      "\n",
      "The following search terms did not yield a direct mapping to a URI:\n",
      "* located on: These terms might not be present in the knowledge graph as they seem to be prepositions or phrases that are not directly related to a specific entity. The detected URIs for this term appear to be related to properties or relationships rather than entities.\n",
      "* serves: These terms might not be present in the knowledge graph as they seem to be verbs or phrases that are not directly related to a specific entity. The detected URIs for this term appear to be related to properties or relationships rather than entities.\n",
      "\n",
      "Note that the context of the search terms is a text describing a railway station in the United Kingdom. The mappings are based on the most similar rdfs:label search results provided by the URI detection tool. If a search term has multiple possible mappings, I have chosen the one that seems most relevant based on the context.\n",
      "\n",
      "\n",
      "Agent ID: result_formatting_agent\n",
      "Instruction: \n",
      "Result: \n",
      "http://www.wikidata.org/entity/Q5170476 http://www.wikidata.org/locatedOn http://www.wikidata.org/entity/Q7653559 .\n",
      "http://www.wikidata.org/entity/Q5170476 http://www.wikidata.org/serves http://www.wikidata.org/entity/Q1236511 .\n",
      "http://www.wikidata.org/entity/Q1236511 http://www.wikidata.org/locatedIn http://www.wikidata.org/entity/Q145 .\n",
      "\n",
      "\n",
      "\n",
      "Agent Comments:\n",
      "-- START OF OUTPUT --\n",
      "Given the inputs and the goal of processing the provided text into a triple format for closed information extraction using an underlying Knowledge Graph, I will outline a detailed plan. This plan will guide the Agent Instructor through a series of steps, utilizing the available agents to achieve the desired outcome.\n",
      "\n",
      "### Plan Overview\n",
      "\n",
      "1. **Text Preprocessing**: Although not explicitly mentioned, this step is implicitly required for the Entity Extraction Agent to work efficiently. However, since we don't have a specific Text Preprocessing Agent listed, we will proceed with the assumption that the text is ready for entity extraction.\n",
      "\n",
      "2. **Entity Extraction**:\n",
      "   - **Step 1**: Call the Entity Extraction Agent to extract entities from the provided input text.\n",
      "   - **Expected Output**: A list of entities found in the text.\n",
      "\n",
      "3. **Relation Extraction**:\n",
      "   - **Step 2**: Once entities are identified, call the Relation Extraction Agent to extract relations among the entities found in Step 1.\n",
      "   - **Expected Output**: A list of relations between the extracted entities.\n",
      "\n",
      "4. **URI Detection**:\n",
      "   - **Step 3**: For each entity and relation extracted, call the URI Detection Agent to determine if there is an associated entity or relation in the Knowledge Graph.\n",
      "   - **Expected Output**: A list of URIs corresponding to the entities and relations in the Knowledge Graph.\n",
      "\n",
      "5. **Triple Formation**:\n",
      "   - **Step 4**: Using the entities, relations, and their corresponding URIs from Steps 1, 2, and 3, form triples. This step might require some logic to correctly pair entities with relations and ensure the triples are valid and meaningful.\n",
      "\n",
      "6. **Result Formatting**:\n",
      "   - **Step 5**: Finally, call the Result Formatting Agent to summarize the results and output the final triples in the required format.\n",
      "   - **Expected Output**: The final set of triples representing the extracted information in a structured format.\n",
      "\n",
      "### Current Status\n",
      "We are at the beginning of the plan, with no previous steps executed. The next task is to extract entities from the provided input text.\n",
      "\n",
      "### Next Task\n",
      "Call the Entity Extraction Agent with the provided input text: \"Corfe Castle railway station is a station on the Swanage Railway in the village of Corfe Castle, in the United Kingdom.\" \n",
      "\n",
      "We are currently at **Step 1** of the plan.\n",
      " -- END OF OUTPUT --\n",
      "\n",
      "-- START OF OUTPUT --\n",
      "To improve the result, I would like to provide feedback on the current plan and the output of the entity extraction agent.\n",
      "\n",
      "The plan outlined is comprehensive and covers the necessary steps for transforming the input text into triplets for insertion into an underlying knowledge graph. However, the output of the entity extraction agent is empty, which indicates that this step may not have been executed correctly or the input text may not have been properly preprocessed.\n",
      "\n",
      "To push the result to the edge of what an LLM can do, I would suggest the following:\n",
      "\n",
      "1. **Text Preprocessing**: Although it was assumed that the text is ready for entity extraction, it would be beneficial to explicitly preprocess the text to remove any unnecessary characters, convert all text to lowercase, and tokenize the text into individual words or phrases. This step can help improve the accuracy of the entity extraction agent.\n",
      "2. **Entity Extraction**: The entity extraction agent should be called with the preprocessed text as input. The agent should be configured to extract entities such as locations (e.g., Corfe Castle, Swanage Railway, United Kingdom), organizations (e.g., Swanage Railway), and other relevant entities (e.g., railway station).\n",
      "3. **Relation Extraction**: Once the entities are extracted, the relation extraction agent should be called to extract relations among the entities. For example, the relation between Corfe Castle railway station and Swanage Railway can be extracted as \"located on\".\n",
      "4. **URI Detection**: The URI detection agent should be called to determine the associated URIs for each entity and relation extracted. This step can help to identify the corresponding entities and relations in the knowledge graph.\n",
      "5. **Triple Formation**: Using the entities, relations, and their corresponding URIs, the triple formation step can be executed to create valid and meaningful triples. For example, a triple can be formed as (Corfe Castle railway station, located on, Swanage Railway).\n",
      "\n",
      "To improve the output of the entity extraction agent, I would suggest re-executing the agent with the preprocessed text as input. The expected output should include a list of entities such as:\n",
      "\n",
      "* Corfe Castle\n",
      "* Swanage Railway\n",
      "* United Kingdom\n",
      "* Corfe Castle railway station\n",
      "\n",
      "Additionally, the agent should be configured to extract entities with their corresponding types (e.g., location, organization, railway station).\n",
      "\n",
      "Please re-execute the entity extraction agent with the preprocessed text as input and provide the updated output.\n",
      " -- END OF OUTPUT --\n",
      "\n",
      "-- START OF OUTPUT --\n",
      "### Plan Overview\n",
      "\n",
      "Given the inputs and the goal of processing the provided text into a triple format for closed information extraction using an underlying Knowledge Graph, I will outline a detailed plan. This plan will guide the Agent Instructor through a series of steps, utilizing the available agents to achieve the desired outcome.\n",
      "\n",
      "1. **Text Preprocessing**: Although not explicitly mentioned, this step is implicitly required for the Entity Extraction Agent to work efficiently. However, since we don't have a specific Text Preprocessing Agent listed, we will proceed with the assumption that the text is ready for entity extraction or apply basic preprocessing techniques if necessary.\n",
      "\n",
      "2. **Entity Extraction**:\n",
      "   - **Step 1**: Call the Entity Extraction Agent to extract entities from the provided input text.\n",
      "   - **Expected Output**: A list of entities found in the text.\n",
      "\n",
      "3. **Relation Extraction**:\n",
      "   - **Step 2**: Once entities are identified, call the Relation Extraction Agent to extract relations among the entities found in Step 1.\n",
      "   - **Expected Output**: A list of relations between the extracted entities.\n",
      "\n",
      "4. **URI Detection**:\n",
      "   - **Step 3**: For each entity and relation extracted, call the URI Detection Agent to determine if there is an associated entity or relation in the Knowledge Graph.\n",
      "   - **Expected Output**: A list of URIs corresponding to the entities and relations in the Knowledge Graph.\n",
      "\n",
      "5. **Triple Formation**:\n",
      "   - **Step 4**: Using the entities, relations, and their corresponding URIs from Steps 1, 2, and 3, form triples. This step might require some logic to correctly pair entities with relations and ensure the triples are valid and meaningful.\n",
      "\n",
      "6. **Result Formatting**:\n",
      "   - **Step 5**: Finally, call the Result Formatting Agent to summarize the results and output the final triples in the required format.\n",
      "   - **Expected Output**: The final set of triples representing the extracted information in a structured format.\n",
      "\n",
      "### Current Status\n",
      "\n",
      "We have executed **Step 1** of the plan by calling the Entity Extraction Agent. However, the output was empty, suggesting that either the agent did not execute correctly or the input text was not properly preprocessed.\n",
      "\n",
      "### Next Task\n",
      "\n",
      "Given the current status, the next task is to re-execute the Entity Extraction Agent with the provided input text after applying basic preprocessing techniques if necessary. The input text is: \"Corfe Castle railway station is a station on the Swanage Railway in the village of Corfe Castle, in the United Kingdom.\"\n",
      "\n",
      "The expected output should include a list of entities such as:\n",
      "- Corfe Castle\n",
      "- Swanage Railway\n",
      "- United Kingdom\n",
      "- Corfe Castle railway station\n",
      "\n",
      "Additionally, the agent should be configured to extract entities with their corresponding types (e.g., location, organization, railway station).\n",
      "\n",
      "We are currently at **Step 1** of the plan, awaiting the successful execution of the Entity Extraction Agent to proceed with the subsequent steps.\n",
      " -- END OF OUTPUT --\n",
      "\n",
      "-- START OF OUTPUT --\n",
      "### Feedback on the Current Plan and Output\n",
      "\n",
      "The plan outlined is comprehensive and covers the necessary steps for transforming the input text into triplets for insertion into an underlying knowledge graph. The output of the entity extraction agent has improved, including entities such as Corfe Castle railway station, Swanage Railway, Corfe Castle, and United Kingdom.\n",
      "\n",
      "However, to further improve the result and push it to the edge of what an LLM can do, I suggest the following adjustments:\n",
      "\n",
      "1. **Entity Disambiguation**: The entity extraction agent should be configured to disambiguate entities with similar names. For example, Corfe Castle refers to both the village and the railway station. Disambiguating these entities will help in forming accurate triples.\n",
      "\n",
      "2. **Relation Extraction**: The relation extraction agent should be called to extract relations among the entities found in Step 1. For example, the relation between Corfe Castle railway station and Swanage Railway can be extracted as \"located on\". Additionally, the relation between Corfe Castle railway station and Corfe Castle can be extracted as \"serves\".\n",
      "\n",
      "3. **URI Detection**: The URI detection agent should be called to determine the associated URIs for each entity and relation extracted. This step can help to identify the corresponding entities and relations in the knowledge graph.\n",
      "\n",
      "4. **Triple Formation**: Using the entities, relations, and their corresponding URIs, the triple formation step can be executed to create valid and meaningful triples. For example, a triple can be formed as (Corfe Castle railway station, located on, Swanage Railway).\n",
      "\n",
      "5. **Result Formatting**: Finally, the result formatting agent should be called to summarize the results and output the final triples in the required format.\n",
      "\n",
      "### Expected Output\n",
      "\n",
      "The expected output should include a list of triples representing the extracted information in a structured format, such as:\n",
      "\n",
      "* (Corfe Castle railway station, located on, Swanage Railway)\n",
      "* (Corfe Castle railway station, serves, Corfe Castle)\n",
      "* (Corfe Castle, located in, United Kingdom)\n",
      "\n",
      "### Next Task\n",
      "\n",
      "The next task is to execute the relation extraction agent to extract relations among the entities found in Step 1. The input to the relation extraction agent should include the entities extracted by the entity extraction agent, such as Corfe Castle railway station, Swanage Railway, Corfe Castle, and United Kingdom.\n",
      "\n",
      "Please execute the relation extraction agent and provide the updated output.\n",
      " -- END OF OUTPUT --\n",
      "\n",
      "-- START OF OUTPUT --\n",
      "### Plan Overview\n",
      "\n",
      "Given the inputs and the goal of processing the provided text into a triple format for closed information extraction using an underlying Knowledge Graph, I will outline a detailed plan. This plan will guide the Agent Instructor through a series of steps, utilizing the available agents to achieve the desired outcome.\n",
      "\n",
      "1. **Text Preprocessing**: Although not explicitly mentioned, this step is implicitly required for the Entity Extraction Agent to work efficiently. However, since we don't have a specific Text Preprocessing Agent listed, we will proceed with the assumption that the text is ready for entity extraction or apply basic preprocessing techniques if necessary.\n",
      "\n",
      "2. **Entity Extraction**:\n",
      "   - **Step 1**: Call the Entity Extraction Agent to extract entities from the provided input text.\n",
      "   - **Expected Output**: A list of entities found in the text.\n",
      "\n",
      "3. **Relation Extraction**:\n",
      "   - **Step 2**: Once entities are identified, call the Relation Extraction Agent to extract relations among the entities found in Step 1.\n",
      "   - **Expected Output**: A list of relations between the extracted entities.\n",
      "\n",
      "4. **URI Detection**:\n",
      "   - **Step 3**: For each entity and relation extracted, call the URI Detection Agent to determine if there is an associated entity or relation in the Knowledge Graph.\n",
      "   - **Expected Output**: A list of URIs corresponding to the entities and relations in the Knowledge Graph.\n",
      "\n",
      "5. **Triple Formation**:\n",
      "   - **Step 4**: Using the entities, relations, and their corresponding URIs from Steps 1, 2, and 3, form triples. This step might require some logic to correctly pair entities with relations and ensure the triples are valid and meaningful.\n",
      "\n",
      "6. **Result Formatting**:\n",
      "   - **Step 5**: Finally, call the Result Formatting Agent to summarize the results and output the final triples in the required format.\n",
      "   - **Expected Output**: The final set of triples representing the extracted information in a structured format.\n",
      "\n",
      "### Current Status\n",
      "\n",
      "We have executed **Step 1** and **Step 2** of the plan by calling the Entity Extraction Agent and the Relation Extraction Agent. The output of the entity extraction agent includes entities such as Corfe Castle railway station, Swanage Railway, Corfe Castle, and United Kingdom.\n",
      "\n",
      "### Next Task\n",
      "\n",
      "The next task is to execute **Step 3**, which involves calling the URI Detection Agent to determine the associated URIs for each entity and relation extracted. The input to the URI Detection Agent should include the entities and relations extracted by the entity extraction agent and the relation extraction agent.\n",
      "\n",
      "Please execute the URI Detection Agent with the following input:\n",
      "- Entities: Corfe Castle railway station, Swanage Railway, Corfe Castle, United Kingdom\n",
      "- Relations: located on, serves\n",
      "\n",
      "The expected output should include a list of URIs corresponding to the entities and relations in the Knowledge Graph. \n",
      "\n",
      "We are currently at **Step 3** of the plan, awaiting the successful execution of the URI Detection Agent to proceed with the subsequent steps.\n",
      " -- END OF OUTPUT --\n",
      "\n",
      "-- START OF OUTPUT --\n",
      "### Feedback on the Current Plan and Output\n",
      "\n",
      "The plan outlined is comprehensive and covers the necessary steps for transforming the input text into triplets for insertion into an underlying knowledge graph. The output of the entity extraction agent and the URI detection agent has improved, including entities such as Corfe Castle railway station, Swanage Railway, Corfe Castle, and United Kingdom, along with their corresponding URIs.\n",
      "\n",
      "However, to further improve the result and push it to the edge of what an LLM can do, I suggest the following adjustments:\n",
      "\n",
      "1. **Triple Formation**: Using the entities, relations, and their corresponding URIs, the triple formation step can be executed to create valid and meaningful triples. For example, a triple can be formed as (Corfe Castle railway station, located on, Swanage Railway). Another triple can be formed as (Corfe Castle railway station, serves, Corfe Castle).\n",
      "\n",
      "2. **Result Formatting**: Finally, the result formatting agent should be called to summarize the results and output the final triples in the required format.\n",
      "\n",
      "### Expected Output\n",
      "\n",
      "The expected output should include a list of triples representing the extracted information in a structured format, such as:\n",
      "\n",
      "* (Corfe Castle railway station, located on, Swanage Railway)\n",
      "* (Corfe Castle railway station, serves, Corfe Castle)\n",
      "* (Corfe Castle, located in, United Kingdom)\n",
      "\n",
      "### Next Task\n",
      "\n",
      "The next task is to execute the triple formation step using the entities, relations, and their corresponding URIs extracted in the previous steps. The input to the triple formation step should include the entities and relations extracted by the entity extraction agent and the relation extraction agent, along with their corresponding URIs detected by the URI detection agent.\n",
      "\n",
      "Please execute the triple formation step with the following input:\n",
      "- Entities: Corfe Castle railway station, Swanage Railway, Corfe Castle, United Kingdom\n",
      "- Relations: located on, serves\n",
      "- URIs: http://www.wikidata.org/entity/Q5170476, http://www.wikidata.org/entity/Q7653559, http://www.wikidata.org/entity/Q1236511, http://www.wikidata.org/entity/Q145\n",
      "\n",
      "The expected output should include a list of triples representing the extracted information in a structured format. \n",
      "\n",
      "We are currently at **Step 4** of the plan, awaiting the successful execution of the triple formation step to proceed with the subsequent steps.\n",
      " -- END OF OUTPUT --\n",
      "\n",
      "-- START OF OUTPUT --\n",
      "### Plan Overview\n",
      "\n",
      "Given the inputs and the goal of processing the provided text into a triple format for closed information extraction using an underlying Knowledge Graph, I will outline a detailed plan. This plan will guide the Agent Instructor through a series of steps, utilizing the available agents to achieve the desired outcome.\n",
      "\n",
      "1. **Text Preprocessing**: Although not explicitly mentioned, this step is implicitly required for the Entity Extraction Agent to work efficiently. However, since we don't have a specific Text Preprocessing Agent listed, we will proceed with the assumption that the text is ready for entity extraction or apply basic preprocessing techniques if necessary.\n",
      "\n",
      "2. **Entity Extraction**:\n",
      "   - **Step 1**: Call the Entity Extraction Agent to extract entities from the provided input text.\n",
      "   - **Expected Output**: A list of entities found in the text.\n",
      "\n",
      "3. **Relation Extraction**:\n",
      "   - **Step 2**: Once entities are identified, call the Relation Extraction Agent to extract relations among the entities found in Step 1.\n",
      "   - **Expected Output**: A list of relations between the extracted entities.\n",
      "\n",
      "4. **URI Detection**:\n",
      "   - **Step 3**: For each entity and relation extracted, call the URI Detection Agent to determine if there is an associated entity or relation in the Knowledge Graph.\n",
      "   - **Expected Output**: A list of URIs corresponding to the entities and relations in the Knowledge Graph.\n",
      "\n",
      "5. **Triple Formation**:\n",
      "   - **Step 4**: Using the entities, relations, and their corresponding URIs from Steps 1, 2, and 3, form triples. This step might require some logic to correctly pair entities with relations and ensure the triples are valid and meaningful.\n",
      "\n",
      "6. **Result Formatting**:\n",
      "   - **Step 5**: Finally, call the Result Formatting Agent to summarize the results and output the final triples in the required format.\n",
      "   - **Expected Output**: The final set of triples representing the extracted information in a structured format.\n",
      "\n",
      "### Current Status\n",
      "\n",
      "We have executed **Step 1**, **Step 2**, and **Step 3** of the plan by calling the Entity Extraction Agent, the Relation Extraction Agent, and the URI Detection Agent. The output of the entity extraction agent includes entities such as Corfe Castle railway station, Swanage Railway, Corfe Castle, and United Kingdom. The URI detection agent has also provided the corresponding URIs for these entities.\n",
      "\n",
      "### Next Task\n",
      "\n",
      "The next task is to execute **Step 4**, which involves forming triples using the entities, relations, and their corresponding URIs extracted in the previous steps. The input to the triple formation step should include the entities and relations extracted by the entity extraction agent and the relation extraction agent, along with their corresponding URIs detected by the URI detection agent.\n",
      "\n",
      "Please execute the triple formation step with the following input:\n",
      "- Entities: Corfe Castle railway station, Swanage Railway, Corfe Castle, United Kingdom\n",
      "- Relations: located on, serves\n",
      "- URIs: http://www.wikidata.org/entity/Q5170476, http://www.wikidata.org/entity/Q7653559, http://www.wikidata.org/entity/Q1236511, http://www.wikidata.org/entity/Q145\n",
      "\n",
      "The expected output should include a list of triples representing the extracted information in a structured format.\n",
      "\n",
      "We are currently at **Step 4** of the plan, awaiting the successful execution of the triple formation step to proceed with the subsequent steps.\n",
      "\n",
      "After the triple formation step, we will proceed to **Step 5**, which involves calling the Result Formatting Agent to summarize the results and output the final triples in the required format. \n",
      "\n",
      "### Step 5: Result Formatting\n",
      "\n",
      "Once the triples are formed, the final step is to call the Result Formatting Agent. This agent will take the triples formed in Step 4 and format them into the final output required for the task.\n",
      "\n",
      "- **Input to Result Formatting Agent**: The triples formed in Step 4.\n",
      "- **Expected Output**: The final set of triples in a structured format, ready for insertion into the Knowledge Graph.\n",
      "\n",
      "This concludes the plan for processing the given text into a triple format for closed information extraction using an underlying Knowledge Graph.\n",
      " -- END OF OUTPUT --\n",
      "\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "f8df3e536fff8210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:31:10.678932Z",
     "start_time": "2025-03-18T16:31:10.675759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_uri_labels(df): \n",
    "    subjects = []\n",
    "    predicates = []\n",
    "    objects = []\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            subjects.append(entity_set[entity_set[\"entity_uri\"] == row[\"subject_uri\"]][\"entity\"].values[0])\n",
    "        except IndexError:\n",
    "            subjects.append(\"Unknown\")\n",
    "        try:\n",
    "            predicates.append(predicate_set_df[predicate_set_df[\"predicate_uri\"] == row[\"predicate_uri\"]][\"predicate\"].values[0])\n",
    "        except IndexError:\n",
    "            predicates.append(\"Unknown\")\n",
    "        if row[\"object_uri\"] is not None and \"^^\" in row[\"object_uri\"]:\n",
    "            objects.append(row[\"object_uri\"])\n",
    "        else:\n",
    "            try:\n",
    "                objects.append(entity_set[entity_set[\"entity_uri\"] == row[\"object_uri\"]][\"entity\"].values[0])\n",
    "            except IndexError:\n",
    "                objects.append(\"Unknown\")\n",
    "    return pd.concat([df.reset_index(drop=True), pd.DataFrame({\"subject\": subjects, \"predicate\": predicates, \"object\": objects})], axis=1)"
   ],
   "id": "aeeee3f98c33d20d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:14:40.956524Z",
     "start_time": "2025-03-14T09:14:40.934072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef\n",
    "\n",
    "ttl_file_path = \"../../Data/Product_Graph.ttl\"  # Path to your Turtle file\n",
    "\n",
    "# Load the Turtle file into an RDF graph\n",
    "graph = Graph()\n",
    "graph.parse(data=response_state[\"results\"][-1], format=\"turtle\")\n",
    "\n",
    "# Extract triples and convert to a DataFrame\n",
    "data = []\n",
    "namespace_manager = graph.namespace_manager  # Namespace manager for prefix resolution\n",
    "\n",
    "final_result = []\n",
    "for subj, pred, obj in graph:\n",
    "    final_result.append([str(subj), str(pred), str(obj)])\n",
    "    \n",
    "pred_relation_df = pd.DataFrame(final_result, columns=[\"subject_uri\", \"predicate_uri\", \"object_uri\"]).drop_duplicates()\n",
    "doc_relation_df = relation_df[relation_df[\"docid\"] == doc_id][[\"subject_uri\", \"predicate_uri\", \"object_uri\"]]\n",
    "correct_relation_df = pred_relation_df.merge(doc_relation_df[[\"subject_uri\", \"predicate_uri\", \"object_uri\"]], on=[\"subject_uri\", \"predicate_uri\", \"object_uri\"], how=\"inner\")"
   ],
   "id": "78489304aabd4bbb",
   "outputs": [
    {
     "ename": "BadSyntax",
     "evalue": "at line 2 of <>:\nBad syntax (Prefix \"http:\" not bound) at ^ in:\n\"b'\\n'^b'http://www.wikidata.org/entity/Q5170476 http://www.wikidata.'...\"",
     "output_type": "error",
     "traceback": [
      "Traceback \u001B[0;36m(most recent call last)\u001B[0m:\n",
      "\u001B[0;36m  File \u001B[0;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1232\u001B[0;36m in \u001B[0;35muri_ref2\u001B[0;36m\n\u001B[0;31m    ns = self._bindings[pfx]\u001B[0;36m\n",
      "\u001B[0;31mKeyError\u001B[0m\u001B[0;31m:\u001B[0m 'http'\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "Traceback \u001B[0;36m(most recent call last)\u001B[0m:\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3579\u001B[0m in \u001B[1;35mrun_code\u001B[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001B[0m\n",
      "\u001B[0m  Cell \u001B[1;32mIn[71], line 8\u001B[0m\n    graph.parse(data=response_state[\"results\"][-1], format=\"turtle\")\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/graph.py:1551\u001B[0m in \u001B[1;35mparse\u001B[0m\n    raise se\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/graph.py:1542\u001B[0m in \u001B[1;35mparse\u001B[0m\n    parser.parse(source, self, **args)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:2020\u001B[0m in \u001B[1;35mparse\u001B[0m\n    p.loadStream(stream)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:479\u001B[0m in \u001B[1;35mloadStream\u001B[0m\n    return self.loadBuf(stream.read())  # Not ideal\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:485\u001B[0m in \u001B[1;35mloadBuf\u001B[0m\n    self.feed(buf)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:511\u001B[0m in \u001B[1;35mfeed\u001B[0m\n    i = self.directiveOrStatement(s, j)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:530\u001B[0m in \u001B[1;35mdirectiveOrStatement\u001B[0m\n    j = self.statement(argstr, i)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:774\u001B[0m in \u001B[1;35mstatement\u001B[0m\n    i = self.object(argstr, i, r)  # Allow literal for subject - extends RDF\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1487\u001B[0m in \u001B[1;35mobject\u001B[0m\n    j = self.subject(argstr, i, res)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:785\u001B[0m in \u001B[1;35msubject\u001B[0m\n    return self.item(argstr, i, res)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:877\u001B[0m in \u001B[1;35mitem\u001B[0m\n    return self.path(argstr, i, res)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:884\u001B[0m in \u001B[1;35mpath\u001B[0m\n    j = self.nodeOrLiteral(argstr, i, res)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1515\u001B[0m in \u001B[1;35mnodeOrLiteral\u001B[0m\n    j = self.node(argstr, i, res)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1102\u001B[0m in \u001B[1;35mnode\u001B[0m\n    j = self.uri_ref2(argstr, i, res)\u001B[0m\n",
      "\u001B[0m  File \u001B[1;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1240\u001B[0m in \u001B[1;35muri_ref2\u001B[0m\n    self.BadSyntax(argstr, i, 'Prefix \"%s:\" not bound' % (pfx))\u001B[0m\n",
      "\u001B[0;36m  File \u001B[0;32m~/Documents/Uni/Masterarbeit/CIExMAS/.pyvenv311/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1730\u001B[0;36m in \u001B[0;35mBadSyntax\u001B[0;36m\n\u001B[0;31m    raise BadSyntax(self._thisDoc, self.lines, argstr, i, msg)\u001B[0;36m\n",
      "\u001B[0;36m  File \u001B[0;32m<string>\u001B[0;36m\u001B[0m\n\u001B[0;31mBadSyntax\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:14:40.984328Z",
     "start_time": "2025-03-14T09:13:43.784660Z"
    }
   },
   "cell_type": "code",
   "source": "get_uri_labels(pred_relation_df)",
   "id": "7b0699109b149243",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               subject_uri  \\\n",
       "0  http://www.wikidata.org/entity/Q1236511   \n",
       "1  http://www.wikidata.org/entity/Q5170476   \n",
       "2  http://www.wikidata.org/entity/Q5170476   \n",
       "3  http://www.wikidata.org/entity/Q1236511   \n",
       "4  http://www.wikidata.org/entity/Q5170476   \n",
       "5  http://www.wikidata.org/entity/Q1236511   \n",
       "\n",
       "                              predicate_uri  \\\n",
       "0   http://www.wikidata.org/prop/direct/P17   \n",
       "1  http://www.wikidata.org/prop/direct/P276   \n",
       "2  http://www.wikidata.org/prop/direct/P276   \n",
       "3  http://www.wikidata.org/prop/direct/P276   \n",
       "4  http://www.wikidata.org/prop/direct/P361   \n",
       "5  http://www.wikidata.org/prop/direct/P361   \n",
       "\n",
       "                                object_uri                       subject  \\\n",
       "0      http://www.wikidata.org/entity/Q145                  Corfe_Castle   \n",
       "1  http://www.wikidata.org/entity/Q7653559  Corfe_Castle_railway_station   \n",
       "2  http://www.wikidata.org/entity/Q1236511  Corfe_Castle_railway_station   \n",
       "3      http://www.wikidata.org/entity/Q145                  Corfe_Castle   \n",
       "4  http://www.wikidata.org/entity/Q7653559  Corfe_Castle_railway_station   \n",
       "5      http://www.wikidata.org/entity/Q145                  Corfe_Castle   \n",
       "\n",
       "  predicate           object  \n",
       "0   Unknown   United_Kingdom  \n",
       "1   Unknown  Swanage_Railway  \n",
       "2   Unknown     Corfe_Castle  \n",
       "3   Unknown   United_Kingdom  \n",
       "4   Unknown  Swanage_Railway  \n",
       "5   Unknown   United_Kingdom  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_uri</th>\n",
       "      <th>predicate_uri</th>\n",
       "      <th>object_uri</th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1236511</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P17</td>\n",
       "      <td>http://www.wikidata.org/entity/Q145</td>\n",
       "      <td>Corfe_Castle</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q5170476</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P276</td>\n",
       "      <td>http://www.wikidata.org/entity/Q7653559</td>\n",
       "      <td>Corfe_Castle_railway_station</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Swanage_Railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q5170476</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P276</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1236511</td>\n",
       "      <td>Corfe_Castle_railway_station</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Corfe_Castle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1236511</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P276</td>\n",
       "      <td>http://www.wikidata.org/entity/Q145</td>\n",
       "      <td>Corfe_Castle</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q5170476</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P361</td>\n",
       "      <td>http://www.wikidata.org/entity/Q7653559</td>\n",
       "      <td>Corfe_Castle_railway_station</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Swanage_Railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1236511</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P361</td>\n",
       "      <td>http://www.wikidata.org/entity/Q145</td>\n",
       "      <td>Corfe_Castle</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:31:51.736968Z",
     "start_time": "2025-03-18T16:31:51.701919Z"
    }
   },
   "cell_type": "code",
   "source": "get_uri_labels(doc_relation_df)",
   "id": "7bed734f9cb38a8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                              subject_uri  \\\n",
       "0  http://www.wikidata.org/entity/Q677663   \n",
       "1  http://www.wikidata.org/entity/Q677663   \n",
       "2  http://www.wikidata.org/entity/Q677663   \n",
       "3  http://www.wikidata.org/entity/Q677663   \n",
       "4  http://www.wikidata.org/entity/Q677663   \n",
       "\n",
       "                          predicate_uri  \\\n",
       "0  http://www.wikidata.org/entity/P1321   \n",
       "1    http://www.wikidata.org/entity/P27   \n",
       "2   http://www.wikidata.org/entity/P551   \n",
       "3   http://www.wikidata.org/entity/P937   \n",
       "4  http://www.wikidata.org/entity/P1412   \n",
       "\n",
       "                              object_uri          subject  \\\n",
       "0  http://www.wikidata.org/entity/Q36378  Ricardo_Lumengo   \n",
       "1     http://www.wikidata.org/entity/Q39  Ricardo_Lumengo   \n",
       "2   http://www.wikidata.org/entity/Q1034  Ricardo_Lumengo   \n",
       "3     http://www.wikidata.org/entity/Q70  Ricardo_Lumengo   \n",
       "4  http://www.wikidata.org/entity/Q33702  Ricardo_Lumengo   \n",
       "\n",
       "                             predicate          object  \n",
       "0        place of origin (Switzerland)        Fribourg  \n",
       "1               country of citizenship     Switzerland  \n",
       "2                            residence     Biel/Bienne  \n",
       "3                        work location            Bern  \n",
       "4  languages spoken, written or signed  Kongo_language  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_uri</th>\n",
       "      <th>predicate_uri</th>\n",
       "      <th>object_uri</th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q677663</td>\n",
       "      <td>http://www.wikidata.org/entity/P1321</td>\n",
       "      <td>http://www.wikidata.org/entity/Q36378</td>\n",
       "      <td>Ricardo_Lumengo</td>\n",
       "      <td>place of origin (Switzerland)</td>\n",
       "      <td>Fribourg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q677663</td>\n",
       "      <td>http://www.wikidata.org/entity/P27</td>\n",
       "      <td>http://www.wikidata.org/entity/Q39</td>\n",
       "      <td>Ricardo_Lumengo</td>\n",
       "      <td>country of citizenship</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q677663</td>\n",
       "      <td>http://www.wikidata.org/entity/P551</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1034</td>\n",
       "      <td>Ricardo_Lumengo</td>\n",
       "      <td>residence</td>\n",
       "      <td>Biel/Bienne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q677663</td>\n",
       "      <td>http://www.wikidata.org/entity/P937</td>\n",
       "      <td>http://www.wikidata.org/entity/Q70</td>\n",
       "      <td>Ricardo_Lumengo</td>\n",
       "      <td>work location</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q677663</td>\n",
       "      <td>http://www.wikidata.org/entity/P1412</td>\n",
       "      <td>http://www.wikidata.org/entity/Q33702</td>\n",
       "      <td>Ricardo_Lumengo</td>\n",
       "      <td>languages spoken, written or signed</td>\n",
       "      <td>Kongo_language</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:14:40.987990Z",
     "start_time": "2025-03-14T09:13:56.755600Z"
    }
   },
   "cell_type": "code",
   "source": "get_uri_labels(correct_relation_df)",
   "id": "7dff500fcad25db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subject_uri, predicate_uri, object_uri, subject, predicate, object]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_uri</th>\n",
       "      <th>predicate_uri</th>\n",
       "      <th>object_uri</th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(pred_relation_df, doc_id, verbose=False):\n",
    "    doc_relation_df = relation_df[relation_df[\"docid\"] == doc_id][[\"subject_uri\", \"predicate_uri\", \"object_uri\"]]\n",
    "    correct_relation_df = pred_relation_df.merge(doc_relation_df[[\"subject_uri\", \"predicate_uri\", \"object_uri\"]], on=[\"subject_uri\", \"predicate_uri\", \"object_uri\"], how=\"inner\")\n",
    "    precision = len(correct_relation_df) / len(pred_relation_df)\n",
    "    recall = len(correct_relation_df) / len(doc_relation_df)\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1: {f1_score}\")\n",
    "        \n",
    "    return precision, recall, f1_score"
   ],
   "id": "a812ca1c67908e1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate(pred_relation_df, doc_id, verbose=True)",
   "id": "738ea19b2d3c9e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5130c64b298b5fbc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
